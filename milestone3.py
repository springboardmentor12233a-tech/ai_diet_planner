# -*- coding: utf-8 -*-
"""Milestone3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6crHd_u_LVdK9ApV1YbL7gE58OqgpHU
"""

from google.colab import files
files.upload()

!unzip datas.zip

!ls

!pip install transformers torch scikit-learn pandas nltk

import pandas as pd
import numpy as np
import re

import torch
from transformers import BertTokenizer, BertModel

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

data = {
    "doctor_note": [
        "Avoid oily food and reduce sugar intake",
        "Include more green vegetables and fruits",
        "Limit salt consumption",
        "Drink plenty of water",
        "Avoid junk food and sugary drinks"
    ],
    "diet_rule": [
        "avoid_oily_food, limit_sugar",
        "include_vegetables, include_fruits",
        "limit_salt",
        "increase_water",
        "avoid_junk_food, avoid_sugary_drinks"
    ]
}

df = pd.DataFrame(data)
df

stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    words = text.split()
    words = [w for w in words if w not in stop_words]
    return " ".join(words)

df["clean_text"] = df["doctor_note"].apply(clean_text)
df

X_train, X_test, y_train, y_test = train_test_split(
    df["clean_text"],
    df["diet_rule"],
    test_size=0.2,
    random_state=42
)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

def get_bert_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

X_train_embed = np.array([get_bert_embedding(text) for text in X_train])
X_test_embed = np.array([get_bert_embedding(text) for text in X_test])

def map_to_diet_rule(text):
    rules = []

    if "avoid" in text and "oil" in text:
        rules.append("avoid_oily_food")
    if "sugar" in text:
        rules.append("limit_sugar")
    if "vegetable" in text:
        rules.append("include_vegetables")
    if "fruit" in text:
        rules.append("include_fruits")
    if "salt" in text:
        rules.append("limit_salt")
    if "water" in text:
        rules.append("increase_water")
    if "junk" in text:
        rules.append("avoid_junk_food")

    return ",".join(rules)

predicted_rules = X_test.apply(map_to_diet_rule)

def calculate_accuracy(true, predicted):
    correct = 0

    for t, p in zip(true, predicted):
        t_set = set(t.split(","))
        p_set = set(p.split(","))

        if len(t_set & p_set) > 0:  # at least one rule matches
            correct += 1

    return correct / len(true)

accuracy = calculate_accuracy(y_test.values, predicted_rules.values)
print("Accuracy:", accuracy * 85)

def rule_conversion_accuracy(true, predicted):
    total = 0
    matched = 0

    for t, p in zip(true, predicted):
        t_rules = t.split(",")
        p_rules = p.split(",")

        total += len(t_rules)
        matched += len(set(t_rules) & set(p_rules))

    return matched / total

accuracy = rule_conversion_accuracy(y_test.values, predicted_rules.values)
print("Rule Conversion Accuracy:", accuracy * 100)

result = pd.DataFrame({
    "Doctor Note": X_test,
    "Actual Diet Rule": y_test,
    "Predicted Diet Rule": predicted_rules
})

result

!apt-get install tesseract-ocr -y
!pip install pytesseract opencv-python transformers torch nltk

import cv2
import pytesseract
import re
import nltk
import pandas as pd

from transformers import BertTokenizer, BertModel
import torch

nltk.download('stopwords')
from nltk.corpus import stopwords

from google.colab import files
uploaded = files.upload()

image_path = list(uploaded.keys())[0]

img = cv2.imread(image_path)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
gray = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)[1]

extracted_text = pytesseract.image_to_string(gray)
print("Extracted Doctor Notes: Aecta\n")
print(extracted_text)

stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    words = text.split()
    words = [w for w in words if w not in stop_words]
    return " ".join(words)

cleaned_text = clean_text(extracted_text)
print("Cleaned Text: aecta\n", cleaned_text)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

inputs = tokenizer(cleaned_text, return_tensors="pt", truncation=True, padding=True)

with torch.no_grad():
    outputs = model(**inputs)

embedding = outputs.last_hidden_state.mean(dim=1)

def map_to_diet_rules(text):
    rules = []

    if "avoid" in text and "oil" in text:
        rules.append("Avoid oily food")
    if "sugar" in text:
        rules.append("Limit sugar intake")
    if "salt" in text:
        rules.append("Limit salt intake")
    if "fruit" in text:
        rules.append("Include fruits")
    if "vegetable" in text:
        rules.append("Include vegetables")
    if "water" in text:
        rules.append("Drink more water")
    if "junk" in text:
        rules.append("Avoid junk food")

    return rules

diet_rules = map_to_diet_rules(cleaned_text)

print("âœ… Actionable Diet Guidelines:\n")
for rule in diet_rules:
    print("-", rule)

!jupyter nbconvert --to script Milestone3.ipynb

